{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training an Overcooked Policy using Madrona\n",
        "\n",
        "In this notebook, we train an AI in Overcooked using the Multi-Agent PPO algorithm. The full training process can be completed in 2 minutes using our GPU-accelerated implementation of Overcooked, while the original implementation takes about an hour on the same hardware.\n",
        "\n",
        "##Setup\n",
        "Change the runtime type to use a T4 GPU (free colab tier). Run the next 3 cells to install dependencies for the notebook (should take about 1 minute in total)."
      ],
      "metadata": {
        "id": "s-DeGKpTCsOu"
      },
      "id": "s-DeGKpTCsOu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bca83f6",
      "metadata": {
        "scrolled": true,
        "id": "5bca83f6"
      },
      "outputs": [],
      "source": [
        "%pip uninstall -y build\n",
        "\n",
        "!git clone -b overcooked_prebuilt --single-branch https://github.com/bsarkar321/madrona_rl_envs\n",
        "%cd madrona_rl_envs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/madrona_rl_envs/cuda_so_12/libcudart.so.12 /usr/local/cuda/lib64/libcudart.so.12\n",
        "!ln -s /content/madrona_rl_envs/cuda_so_12/libnvrtc.so.12 /usr/local/cuda/lib64/libnvrtc.so.12\n",
        "!ln -s /content/madrona_rl_envs/cuda_so_12/libnvJitLink.so.12 /usr/local/cuda/lib64/libnvJitLink.so.12"
      ],
      "metadata": {
        "id": "fuZwp4pE9cSj"
      },
      "id": "fuZwp4pE9cSj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a971671e",
      "metadata": {
        "id": "a971671e"
      },
      "outputs": [],
      "source": [
        "%pip install -e .\n",
        "%pip install -e PantheonRL/overcookedgym/human_aware_rl/overcooked_ai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff30b5f",
      "metadata": {
        "id": "7ff30b5f"
      },
      "source": [
        "## Training\n",
        "\n",
        "**Restart runtime here!** Then continue running the next blocks.\n",
        "\n",
        "In this section, we will train the AI from scratch using MAPPO. The top score any agent can achieve in the \"simple\" layout is 234. For the set seed of 1, the average self-play score is 230.655 when the policy is non-deterministic, but it gets a score of 234 when it deterministically chooses an action.\n",
        "\n",
        "Note that different seeds may result in different converged values, typically between 197 and 234. However, there are some cases when the score for the deterministic policy is very low, which occurs when the policy is in between different locally optimal conventions at the end of training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/madrona_rl_envs"
      ],
      "metadata": {
        "id": "rQKD7REV5NxU"
      },
      "id": "rQKD7REV5NxU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e630cd",
      "metadata": {
        "id": "08e630cd"
      },
      "outputs": [],
      "source": [
        "%env MADRONA_MWGPU_KERNEL_CACHE=/content/madrona_rl_envs/caches/madrona_over_cache\n",
        "import os\n",
        "from train.MAPPO.main_player import MainPlayer\n",
        "\n",
        "from train.config import get_config\n",
        "from pathlib import Path\n",
        "\n",
        "from train.env_utils import generate_env\n",
        "\n",
        "from train.partner_agents import CentralizedAgent\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30118915",
      "metadata": {
        "id": "30118915"
      },
      "outputs": [],
      "source": [
        "args = get_config().parse_args(\"\")\n",
        "args.num_env_steps = 8000000\n",
        "args.pop_size = 1\n",
        "args.episode_length = 200\n",
        "args.env_length = 200\n",
        "args.env_name = \"overcooked\"\n",
        "args.seed = 1\n",
        "args.over_layout = \"simple\"\n",
        "args.run_dir = \"sp\"\n",
        "args.restored = 0\n",
        "args.cuda = True\n",
        "\n",
        "args.n_rollout_threads = 800\n",
        "args.ppo_epoch = 7\n",
        "args.layer_N = 1\n",
        "args.hidden_size = 64\n",
        "args.lr = 1e-2\n",
        "args.critic_lr = 1e-2\n",
        "args.entropy_coef = 0.0\n",
        "args.linear_lr_decay = False\n",
        "\n",
        "args.use_baseline = False\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba74bd5",
      "metadata": {
        "id": "2ba74bd5"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() and args.cuda else 'cpu'\n",
        "print(device)\n",
        "\n",
        "envs = generate_env(args.env_name, args.n_rollout_threads, args.over_layout, use_env_cpu=(device=='cpu'), use_baseline=args.use_baseline)\n",
        "\n",
        "args.hanabi_name = args.over_layout if args.env_name == 'overcooked' else args.env_name\n",
        "\n",
        "run_dir = (\n",
        "        \"train/\"\n",
        "        + args.hanabi_name\n",
        "        + \"/results/\"\n",
        "        + (args.run_dir)\n",
        "        + \"/\"\n",
        "        + str(args.seed)\n",
        "    )\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "with open(run_dir + \"/\" + \"args.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
        "    file.write(str(args))\n",
        "config = {\n",
        "    'all_args': args,\n",
        "    'envs': envs,\n",
        "    'device': device,\n",
        "    'num_agents': 2,\n",
        "    'run_dir': Path(run_dir)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6ce1ef7",
      "metadata": {
        "scrolled": true,
        "id": "d6ce1ef7"
      },
      "outputs": [],
      "source": [
        "# If you want to rerun this, remember to also run the two cells above so the environment is regenerated\n",
        "# If you encounter an out of memory error, restart the runtime and start from the cell after \"Restart runtime here!\" above.\n",
        "start = time.time()\n",
        "ego = MainPlayer(config)\n",
        "partner = CentralizedAgent(ego, 1)\n",
        "envs.add_partner_agent(partner)\n",
        "ego.run()\n",
        "end = time.time()\n",
        "print(f\"Total time taken: {end - start} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc31582",
      "metadata": {
        "id": "3cc31582"
      },
      "source": [
        "# Testing the Trained Policy\n",
        "\n",
        "By default, we run the \"deterministic\" version of the policy, which results in a score of 234, the highest possible value in the simple environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d90451",
      "metadata": {
        "id": "e1d90451"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    def __init__(self, actor):\n",
        "        super(Policy, self).__init__()\n",
        "\n",
        "        self.base = actor.base.cnn.cnn\n",
        "        self.act_layer = actor.act\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x.to(dtype=torch.float)\n",
        "        x = self.base(x.permute((0, 3, 1, 2)))\n",
        "        x = self.act_layer(x, deterministic=True)\n",
        "        return x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9080b5ed",
      "metadata": {
        "id": "9080b5ed"
      },
      "outputs": [],
      "source": [
        "run_dir = Path(run_dir)\n",
        "args.model_dir = str(run_dir / 'models')\n",
        "\n",
        "config = {\n",
        "    'all_args': args,\n",
        "    'envs': envs,\n",
        "    'device': device,\n",
        "    'num_agents': 2,\n",
        "    'run_dir': run_dir\n",
        "}\n",
        "\n",
        "ego = MainPlayer(config)\n",
        "ego.restore()\n",
        "torch_network = Policy(ego.policy.actor)\n",
        "\n",
        "actions = torch.zeros((2, args.n_rollout_threads, 1), dtype=int, device=device)\n",
        "\n",
        "state1, state2 = envs.n_reset()\n",
        "scores = torch.zeros(args.n_rollout_threads, device=device)\n",
        "for i in range(args.env_length):\n",
        "    actions[0, :, :] = torch_network(state1.obs)\n",
        "    actions[1, :, :] = torch_network(state2.obs)\n",
        "    (state1, state2), reward, _, _ = envs.n_step(actions)\n",
        "    scores += reward[0, :]\n",
        "score_vals, counts = torch.unique(scores, return_counts=True)\n",
        "\n",
        "# printing scores here\n",
        "print({x.item() : y.item() for x, y in zip(score_vals, counts)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00aea08b",
      "metadata": {
        "id": "00aea08b"
      },
      "source": [
        "## Viewing and Interacting with Trained Agents\n",
        "\n",
        "The next 3 cells convert the policy you just trained into an tensorflowjs model, allowing you to visualize your trained agent on-device through the javascript version of Overcooked. The game runs for 40 seconds, so the best score we were able to observe is 120, corresponding to 6 dishes served.\n",
        "\n",
        "If you would like to see AI agents playing in other layouts, change the args.over_layout in the cells above, and retrain the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507363e5",
      "metadata": {
        "id": "507363e5"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "class SimplePolicy(nn.Module):\n",
        "\n",
        "    def __init__(self, actor):\n",
        "        super(SimplePolicy, self).__init__()\n",
        "\n",
        "        self.base = actor.base.cnn.cnn\n",
        "        self.act_layer = actor.act.action_out.linear\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.base(x.permute((0, 3, 1, 2)))\n",
        "        x = self.act_layer(x)\n",
        "        return nn.functional.softmax(x, dim=1)\n",
        "\n",
        "args.n_rollout_threads = 1\n",
        "\n",
        "s_envs = generate_env(args.env_name, args.n_rollout_threads, args.over_layout)\n",
        "\n",
        "args.hanabi_name = args.over_layout if args.env_name == 'overcooked' else args.env_name\n",
        "\n",
        "torch_network = SimplePolicy(ego.policy.actor)\n",
        "\n",
        "vobs, _ = s_envs.n_reset()\n",
        "obs = vobs.obs.to(dtype=torch.float)\n",
        "\n",
        "print(\"*\" * 20, \" TORCH \", \"*\" * 20)\n",
        "\n",
        "print(torch_network)\n",
        "\n",
        "print(obs.shape)\n",
        "\n",
        "print(torch_network(obs))\n",
        "\n",
        "print(\"*\" * 20, \" ONNX \", \"*\" * 20)\n",
        "onnx_model_path = str(run_dir / \"models\" / f\"MAPPO_{args.over_layout}_agent.onnx\")\n",
        "\n",
        "input_name = 'input'  # 'ppo_agent/ppo2_model/Ob'\n",
        "\n",
        "torch.onnx.export(torch_network,\n",
        "                  obs,\n",
        "                  onnx_model_path,\n",
        "                  export_params=True,\n",
        "                  input_names=[input_name],\n",
        "                  output_names=['output'],\n",
        "                  opset_version=10)\n",
        "\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "\n",
        "print(onnx_model.graph.input[0])\n",
        "\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "print(\"*\" * 20, \" TF \", \"*\" * 20)\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_model_dir = str(run_dir / 'models' / f'MAPPO_{args.over_layout}_agent')\n",
        "tf_rep.export_graph(tf_model_dir)\n",
        "\n",
        "tfjs_model_dir = f\"overcooked_demo/static/assets/MAPPO_{args.over_layout}_agent\"\n",
        "tfjs_convert_command = f\"\"\"tensorflowjs_converter\n",
        "                 --input_format=tf_saved_model\n",
        "                 --output_format=tfjs_graph_model\n",
        "                 --signature_name=serving_default\n",
        "                 --saved_model_tags=serve\n",
        "                 \"{tf_model_dir}\"\n",
        "                 \"{tfjs_model_dir}\"\n",
        "                 \"\"\"\n",
        "tfjs_convert_command = \" \".join(tfjs_convert_command.split())\n",
        "\n",
        "# This should only take about a minute in total. If it runs for longer, you may have to restart the runtime.\n",
        "# In this case, you would need to run all the cells after the one that says \"Restart runtime here!\" but you\n",
        "# can skip the cell for actually training the policy (since it is already saved to disk).\n",
        "os.system(tfjs_convert_command)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run this ONCE for a runtime instance\n",
        "\n",
        "# code to display on colab from https://stackoverflow.com/a/61504116\n",
        "%cd /content/madrona_rl_envs/overcooked_demo\n",
        "get_ipython().system_raw('python3 -m http.server 8888 &')\n",
        "%cd /content/madrona_rl_envs"
      ],
      "metadata": {
        "id": "eD3PGkTrpU2r"
      },
      "id": "eD3PGkTrpU2r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "def show_port(port, height=900):\n",
        "  display(Javascript(\"\"\"\n",
        "  (async ()=>{\n",
        "    fm = document.createElement('iframe')\n",
        "    fm.src = await google.colab.kernel.proxyPort(%s)\n",
        "    fm.width = '95%%'\n",
        "    fm.height = '%d'\n",
        "    fm.frameBorder = 0\n",
        "    document.body.append(fm)\n",
        "  })();\n",
        "  \"\"\" % (port, height) ))\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8888)\"))\n",
        "\n",
        "# displays the game in-line; you may have to wait for over a minute after this\n",
        "# line runs before it appears, since it is running through javascript on your\n",
        "# device\n",
        "show_port(8888)"
      ],
      "metadata": {
        "id": "mt-NvAou2Kk9"
      },
      "id": "mt-NvAou2Kk9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}